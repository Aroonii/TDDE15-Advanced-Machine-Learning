---
title: "Lab1"
author: "Arun Uppugunduri"
date: '2020-09-13'
output: word_document
---
In this assignment we are working with classifying the asia dataset. The task si to classify the variable S smoker given the other. The classification is to be solved by fitting the data to a bayesian network. We are trying to classify the variable S = Smoker [yes/no] by looking at the observations of the other nodes.
```{r, echo = FALSE, eval = TRUE}
library(bnlearn)
data(asia)
head(asia, 5)
```
Below can be seen the true structure of the network. The first part of this lab lies in applying the hill-climbing algorithm in order to learn this network structure
```{r, eval = TRUE}
variables = colnames(asia)
true_network = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]", ordering = variables)
plot(true_network, main = "True network")
```
*Task 1*
*Show that multiple runs of the hill-climbing algorithms can return non-equivalent Bayesian network (BN) structures*
The hill-climbing algorithm is a iterative algorithm which adds/eliminates edges between nodes in order to find the optimal tree. The optimal tree chosen is that which has the highest posterior porbability. Using the hill climbing algorithm there is a risk of getting stuck in a local optimum. Depending on extra input like initial structure, score to be used and amount of restarts we can thereby end up with non-equivalent bayesian network structure. This means that that there is no unambiguity in the model.
```{r, eval = TRUE}
#Network 1
network1 = hc(asia, restart = 1, score = "bic")
plot(network1, main = "network 1")
cpdag(network1)
#Network 2
network2 = hc(asia, restart = 100, score = "bic")
plot(network2, main = "network 2")
cpdag(network2)
```
```{r, eval = TRUE, echo = FALSE}
cat("Score for network 1, restart = 1", score(network1, asia))
```
```{r, eval = TRUE, echo = FALSE}
cat("Score for network 2, restart = 100", score(network2, asia))
```
```{r, eval = TRUE, echo = FALSE}
print(all.equal(network1, network2))
```
By changing the value for restart we modify how many random restarts of the algorithm should be used. In this example we can howver conclude that it does NOT result in non-equivalent network strutures. We know this by comparing the structure. Since only the direction of an edge is what differs this does not change any form of independence/dependence. This can also be seen by comparing the scores. Since the scooring criteria assign the same score to equivalent structures.
```{r, eval = TRUE}
#Network 3
network3 = hc(asia, score = "aic", restart = 100)
plot(network3)
cpdag(network3)
#Network 4
network4 = hc(asia, score = "bic", restart = 100)
plot(network4)
cpdag(network4)
#Comparing network 3 and 4
all.equal(network3, network4)
```
```{r, eval = TRUE, echo = FALSE}
cat("Score for network 3, score = aic, restart = 100 => Score: ",
score(network3, asia))
```
```{r, eval = TRUE, echo = FALSE}
cat("Score for network 4, score = bic restart = 100, => Score:", score(network4, asia))
```
In this step i compared the resutls from using different scores. By looking at the new plots produced we can easily conclude that the two models created are different , in other words non-equivalent. This can also be stated by looking at their scores. The best values to select in this case would thereby be network 4 using the default bic Bayesian Information Criterion score. This is because it has the highest score but we also notice that it's graph show an independence of A. Since the hill climbing algorithm does not produce any false independences then we know that this is something which we can trust and there is no use in modelling extra unesseary parameters.
*Task 2*
*Learn a BN with 80 % of the data. Learn both its structure and its parameters. Use any learning algorithm and setting that you consider appropriate. Use the BN learned to classify the remaining20 % test data into two classes S = [yes/no]. In other words, compute the posterior probabilitydistribution of S for each case and classify it in the most likely class*
*True network*
*dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]").*
I learn the Bayesian network structure by applying the hill climbing algorithm. I use the settings which in the previous step resulted in the best score.
```{r, eval = TRUE, echo = FALSE}
set.seed(12345)
n = nrow(asia)
id = sample(n, floor(n*0.8))
train = asia[id,]
test = asia[-id,]
bn_network = hc(train, score = "bic", restart = 100)
plot(bn_network, main = "learned network")
```
Then we creat the true network structue using the given structure
```{r, eval = TRUE}
true_network = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]", ordering = variables)
plot(true_network, main = "True network")
```
Having created the network we then fit the parameters of the bayesian network conditional on its structure. We do this by fitting the train data to the netork structure
```{r, eval = TRUE}
bn_fit = bn.fit(bn_network,
data = train)
true_bn_fit = bn.fit(true_network,
data = train)
```
In order to make inference we have to convert the fit to a grain and compile it. The function as.grain conversts the bn.fit object to a grain. The grain class stores a fitted bayesian network as a list of conditional
probability tables. This makes it possible for setEvidence and querygrain to perform posterior inference via belief propagation. The grain does not allow conditional probabilities to be NaN. This happens when estimationg them via a maximum likelihood and some parent ocnfigurations are not observed in the data. Grain will then replace NaN with uniform distribution. In this case the data is however complete and this is not a problem which we have to take into account.
```{r, eval = TRUE}
#Convert to a grain object
bn_fit_gRain = as.grain(bn_fit)
#For the true network
bn_fit_true = as.grain(true_bn_fit)
```
```{r, eval = TRUE}
library(gRain)
#Compile the BN which means creating teh junction tree and establishing clique potentials
junction_tree = compile(bn_fit_gRain)
#For the true network
junction_tree_true = compile(bn_fit_true)
```
In order to be able to make predictions on the new test data we had to implement an own prediction function. In this function we make use of the funcitons setEvidence and querygrain. Using setEvidence we extract the conditional probability tables for a node(s) which can be in states X. What the function does is that it takes all the data and assigns it to the observation nodes. THis means that the node we are tring to classify should not be included. Once this is extracted we can then apply querygrain in order to extract the conditional for a node (prediction node) given the evidence extracted above. Using this conditional probability we then assign each observation S = "yes" if the probability for p(S) = "yes" > 0.5
```{r, eval = TRUE}
predict_test = function(BN_model, testData, observation_nodes, prediction_variable){
prediction = rep(0, length(testData))
for(i in 1:nrow(testData)){
node_state = NULL
for (k in observation_nodes){
node_state[k] = if(testData[i,k] == "yes") "yes" else "no"
}
#Assign the new data points for each observation to a node
# observation nodes should be all the nodes except the conditional node we are doing inference on
# In this case this is S
evidence = setEvidence(object = BN_model,
nodes = observation_nodes,
states = node_state)
#obtain the conditional distribution for a node given the evidence obtained above. In this case we get the conditional
#distribution for S which is the prediction variable
conditional_distribution = querygrain(object = evidence,
nodes = prediction_variable)$S
#For each observation (row in the data set) we classify the prediction variable as yes/no depending on what
#probability is larger
prediction[i] = if(conditional_distribution["yes"] > 0.5) "yes" else "no"
}
return(prediction)
}
test_prediction = predict_test(BN_model = junction_tree,
testData = test,
observation_nodes = c("A", "T", "L", "B", "E", "X", "D"),
prediction_variable = c("S"))
table(test_prediction, test$S)
test_prediction_true = predict_test(BN_model = junction_tree_true,
testData = test,
observation_nodes = c("A", "T", "L", "B", "E", "X", "D"),
prediction_variable = c("S"))
table(test_prediction_true, test$S)
```
We notice that the prediction using the true structure vs the learned structure results in the same confusion matrix.
*Task 3*
*The same task as before but that the classification of S should now be made given observations only for the so called Maakov bnlanket of S, i.e its parents plus its children plus the parents of its children minus S itself*
Extract Marakov blaket from the fitted network structure generated by the hill climb algorithm as well as the fitted network structure created from the true model
```{r, eval = FALSE}
mb_hc = mb(x = bn_fit,
node = c("S"))
mb_true = mb(x = true_bn_fit,
node = c("S"))
```
```{r, eval = TRUE}
mb_hc = mb(x = bn_fit,
node = c("S"))
mb_true = mb(x = true_bn_fit,
node = c("S"))
#Learned network
cat(mb_hc)
#True network
cat(mb_true)
```
The resulting marakov blanket for the two networks is as we can see actually the same. This can also be viewed and verified through the graph where the nodes L and B are the only ones that are connected to prediction variable S. We then make the new predictions but only using the marakov blanket as the obseration nodes. This can be seen as that we are only using certain nodes and certain data to classify S  since the other have no relation to S.
```{r, eval = TRUE}
#Learned network structure
table(prediction = test_prediction_mb, observation =  test$S)
```
```{r, eval = FALSE}
#True network structure
table(prediction = test_prediction_true, observation = test$S)
```
Using the markov blanket we only make computations using the data belonging to the nodes B and L since they are
the parents/children. We see that the resulting matrix is the same and this makes sense. By looking at the graph we can see that the node S is independent from all the nodes given B, L. Adding thhe other nodes would therfore only create more computation but would not gain any more information.
*Task 4*
*Repeat the previous excercise using a naiveBayes classifier, i.e the predictive variables are independent given the class variable*
Create an bayesian network having structure as if all its predictive variables are independent given the class variable.
```{r, eval = TRUE}
BN_bayes =  model2network("[S][A|S][T|S][L|S][B|S][E|S][X|S][D|S]", ordering = variables)
plot(BN_bayes, main = "NaiveBayes")
plot(true_network, main = "True network")
```
Having created the network we do the same precedures as before of fitting the parameters, converting to grain and compiling the junction tree. Finally we
make the prediction using the new naivebayes model and compare it to the true model.
```{r, eval = FALSE}
#Naive bayes classifier
table(Observartion = test$S, Classification = prediction_bayes)
mean(test$S != prediction_bayes)
```
```{r, eval = FALSE}
#True network structure
table(Observartion = test$S, Classification = prediction_true)
mean(test$S != prediction_true)
```
*Task 5*
*Explain why you obtain the same or different results in the exercise 2-4.*
In task 2 where we compared the learned structure using the default score "bic" with restart = 100 we ended up with confusion matrices which where identical. This is due to the fact that S only have the nodes B and L as its marakov blanket. The only difference which could be seen when comparing the two plots was for edges between nodes which where not connected to S. Furthermore we saw another example of this is task 3 when only the marakov blanket was used as the observed variables. Since here, it produced the exakt same confusion matrices yet again. This would suggest that this is a result which is reached either way if we specify the marakov blanket from before or not.
Comparing the naiveBayes network with the true network we can se quite large differences in the structure. When comparing the confusion matrice produced and its missclassification rate we can observe that the naivebayes classifier does a worse job that our original model. This could be explained by the fact that the assumtion of independence between prediction variables does not hold. Looking at the plot for the real network we can see that there are variables which would not be independent gives S.
Comparing
# Code
```{r, eval = FALSE, echo = TRUE}
#Install the necessary
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
install.packages("bnlearn")
BiocManager::install("RBGL")
#Learned network structure
table(prediction = test_prediction_mb, observation =  test$S)
#Learned network structure
table(prediction = test_prediction_mb, observation =  test$S)
library(bnlearn)
data(asia)
head(asia, 5)
variables = colnames(asia)
true_network = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]", ordering = variables)
plot(true_network, main = "True network")
cat("Score for network 1, restart = 1", score(network1, asia))
cat("Score for network 2, restart = 100", score(network2, asia))
print(all.equal(network1, network2))
#Network 3
network3 = hc(asia, score = "aic", restart = 100)
plot(network3)
cpdag(network3)
#Network 4
network4 = hc(asia, score = "bic", restart = 100)
plot(network4)
cpdag(network4)
#Comparing network 3 and 4
all.equal(network3, network4)
cat("Score for network 3, score = aic, restart = 100 => Score: ",
score(network3, asia))
cat("Score for network 4, score = bic restart = 100, => Score:", score(network4, asia))
set.seed(12345)
n = nrow(asia)
id = sample(n, floor(n*0.8))
train = asia[id,]
test = asia[-id,]
bn_network = hc(train, score = "bic", restart = 100)
plot(bn_network, main = "learned network")
true_network = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]", ordering = variables)
plot(true_network, main = "True network")
bn_fit = bn.fit(bn_network,
data = train)
true_bn_fit = bn.fit(true_network,
data = train)
#Convert to a grain object
bn_fit_gRain = as.grain(bn_fit)
#For the true network
bn_fit_true = as.grain(true_bn_fit)
library(gRain)
#Compile the BN which means creating teh junction tree and establishing clique potentials
junction_tree = compile(bn_fit_gRain)
#For the true network
junction_tree_true = compile(bn_fit_true)
#Convert to a grain object
bn_fit_gRain = as.grain(bn_fit)
#For the true network
bn_fit_true = as.grain(true_bn_fit)
predict_test = function(BN_model, testData, observation_nodes, prediction_variable){
prediction = rep(0, length(testData))
for(i in 1:nrow(testData)){
node_state = NULL
for (k in observation_nodes){
node_state[k] = if(testData[i,k] == "yes") "yes" else "no"
}
#Assign the new data points for each observation to a node
# observation nodes should be all the nodes except the conditional node we are doing inference on
# In this case this is S
evidence = setEvidence(object = BN_model,
nodes = observation_nodes,
states = node_state)
#obtain the conditional distribution for a node given the evidence obtained above. In this case we get the conditional
#distribution for S which is the prediction variable
conditional_distribution = querygrain(object = evidence,
nodes = prediction_variable)$S
#For each observation (row in the data set) we classify the prediction variable as yes/no depending on what
#probability is larger
prediction[i] = if(conditional_distribution["yes"] > 0.5) "yes" else "no"
}
return(prediction)
}
test_prediction = predict_test(BN_model = junction_tree,
testData = test,
observation_nodes = c("A", "T", "L", "B", "E", "X", "D"),
prediction_variable = c("S"))
table(test_prediction, test$S)
test_prediction_true = predict_test(BN_model = junction_tree_true,
testData = test,
observation_nodes = c("A", "T", "L", "B", "E", "X", "D"),
prediction_variable = c("S"))
table(test_prediction_true, test$S)
mb_hc = mb(x = bn_fit,
node = c("S"))
mb_true = mb(x = true_bn_fit,
node = c("S"))
mb_hc = mb(x = bn_fit,
node = c("S"))
mb_true = mb(x = true_bn_fit,
node = c("S"))
#Learned network
cat(mb_hc)
#True network
cat(mb_true)
#Learned network structure
table(prediction = test_prediction_mb, observation =  test$S)
test_prediction_mb = predict_test(BN_model = junction_tree,
testData = test,
observation_nodes = mb_hc,
prediction_variable = c("S"))
table(test_prediction_mb, test$S)
mean(test_prediction_mb != test$S)
mean(test_prediction_true_mb != test$S)
#For the true graph
test_prediction_true_mb = predict_test(BN_model = junction_tree_true,
testData = test,
observation_nodes = mb_true,
prediction_variable = c("S"))
table(test_prediction_true, test$S)
mean(test_prediction_true_mb != test$S)
#True network structure
#For the true graph
test_prediction_true_mb = predict_test(BN_model = junction_tree_true,
testData = test,
observation_nodes = mb_true,
prediction_variable = c("S"))
table(test_prediction_true, test$S)
mean(test_prediction_true_mb != test$S)
